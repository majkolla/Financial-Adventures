{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4ff9b5",
   "metadata": {},
   "source": [
    "# Data driven ML method \n",
    "\n",
    "Instead of pricing them using logic and computations as we've explored, I'll make assumptions on the data and instead build data driven models. I'll take some inspiration from  [Ghassane Benrhmach, Khalil Namir, Jamal Bouyaghroumni, Abdelwahed Namir\n",
    "](https://www.researchgate.net/publication/353175680_FINANCIAL_TIME_SERIES_PREDICTION_USING_WAVELET_AND_ARTIFICIAL_NEURAL_NETWORK) and expand on some of their ideas. I'll summerize what they did and then potential changes I'll make. To see how useful the product is, i'll try to do the same comparison with an ARIMA model, and see how my version does. \n",
    "\n",
    "They used a DWT to decompose each price series into an approximation component + details. Then they used a ANN model and trained on the coeff. They used inverse DWT to get the forecast of every approximation comp. + details. \n",
    "\n",
    "## Decomposition \n",
    "One simple improvement is to use SWT instead of using DWT. We can get into the details of SWT, for now I'll leave it at the fact that the fact that shift invariance is important and given that we can look at more volitle markets we may actually gain a lot from moving to a SWT. We also have a 1-1 mapping to the time domain which also is very useful for further analysis. \n",
    "\n",
    "## Network \n",
    "So the choice of network is intersting, we can implement an multi-scale network, which can be intersting given that we get shared learning across all scales. This may work to our advantage since we then might be able to see how differnt details or approximations affect each other. To understand why we have to look into how a normal encoder would work. It would simply mix features from different resolutions without care for the context of that resolution. As you can imagine, from a picture zooming in would give better edges but be more noisy and harder to deduce contex. While zooming out would give more global context, with less noise but we lose the finer structures of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc44d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9be4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_series(symbol=\"CL=F\", start=\"2015-01-02\", end=\"2025-01-02\", col=\"Adj Close\"):\n",
    "    \"\"\"\n",
    "    Download a price series with yfinance. Switch `symbol` to whatevs \n",
    "    \"\"\"\n",
    "    df = yf.download(symbol, start=start, end=end, progress=False)\n",
    "    series = df[col].dropna()\n",
    "    series.name = symbol\n",
    "    return series\n",
    "\n",
    "\n",
    "def swt_decompose(values: np.ndarray, level: int = 3, wavelet: str = \"db4\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Stationary Wavelet Transform. Returns an array shaped\n",
    "    (channels, T) where channels = level * 2 (cA_i, cD_i for i = 1..level)\n",
    "    \"\"\"\n",
    "    coeffs = pywt.swt(values, wavelet, level=level)\n",
    "    cA = [c[0] for c in coeffs]\n",
    "    cD = [c[1] for c in coeffs]\n",
    "    return np.vstack(cA + cD)  # (channels, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34658",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "*Consider an m variate time series* let's say: $\\bar{x}_t = (x_t^{(1)}, x_t^{(2)}, \\dots, x_t^{(m)})\\in \\mathbb{R}^m$. The point here is we have a feature vector with the price, volume volatility etc. In principle we can see this as a matrix or 2D tensor, after creating the patches, these will essentially be the amount of data we assume to be dependent of each other. One intuitive way to see it is basically as how much we zoom into the picture, this will bring us the different resolutions that we're going to look into further.  Consider this 2D tensor, $\\mathbf{X}_i = [\\mathbf{x}_{(i-1)P+1}, \\dots, \\mathbf{x}{iP}]\\in \\mathbb{R}^{P\\times m}$, where m is the amount of features we have in our vector, i is going to be $i = 1, \\dots, T/P$. \n",
    "We are then going to flatten this into a $\\mathbb{R}^{Pm}$ vector. Here comes the *learning* part. We're going to do a linear projection that is going to be learned to find the best weights. Essentially, $\\mathbf{z_i} = \\mathbf{W_E} \\text{vec}(\\mathbf{X_i}) + \\mathbf{b_E}$. \n",
    "\n",
    "Now we can look into the scaling tokens, with a set of tokens that we create, we can then look into different types of linear projections, into three different spaces, for example $K, Q, V$. There is a point of doing it this way, essentially we want to first understand what feature pattern we care about, which is the $Q$ then what feature pattern token j contains and finally what information is stored at token j. We can then compute a similarity score and then compute $A_{ij}$ whic simply is how much of token j contributes to token i. \n",
    "\n",
    "We define this attention matrix as such : $A = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}+ B) \\in \\mathbb{R}^{m\\times m}$. \n",
    "\n",
    "We can mix these values by letting the new tokens be: $Z' = AV$, this implies that each token i becomes a weighted average of the valuje vectors from all tokens in its own window. And finally we can remap to the original channel width with: $\\tilde{Z} = Z'W_O \\in \\mathbb{R}^{M\\times C}$\n",
    "\n",
    "\n",
    "*There is a lot more details and proofs going into each decision of the model, since i am excited to implement stuff to see how well they work rather than reviewing my logic ill work on implementations instead*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ad0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to implement  ashifted window attention for a 1 dimensional time series\n",
    "we do this by: \n",
    "- patch embedding \n",
    "- window self attention \n",
    "- we going to create two swin (similar) blocks to show the flow Q,K,V -> A -> Z' (which i did prove)\n",
    "- then we're going to create simple forecasting\n",
    "\"\"\"\n",
    "\n",
    "def window_partition_1d(x: torch.Tensor, window_size: int):\n",
    "    \"\"\"Split sequence into 1D windows (picture analogy works here)\n",
    "    Args:\n",
    "        x: Tensor[B, L, C]\n",
    "        window_size: int\n",
    "    Returns:\n",
    "        windows: Tensor[B, num_win, M, C] where M = window_size\n",
    "    \"\"\"\n",
    "    B, L, C = x.shape\n",
    "    assert L % window_size == 0, \"Sequence length must be multiple of window.\" \n",
    "    x = x.view(B, L // window_size, window_size, C)\n",
    "    return x  # type: (B, num_win, M, C)\n",
    "\n",
    "\n",
    "def window_reverse_1d(windows: torch.Tensor, window_size: int):\n",
    "    \"\"\"Inverse of window_partition_1d.\"\"\"\n",
    "    B, num_win, M, C = windows.shape\n",
    "    x = windows.reshape(B, num_win * window_size, C)\n",
    "    return x  # (B, L, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
